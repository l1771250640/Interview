# 深度学习优化算法


## 1. SGD
>SGD每次计算mini-batch的梯度，对参数进行更新。

### 1.1batch GD（批量梯度下降）
每次更新参数使用所有样本。优点：全局最优解；易于并行实现。缺点：样本量大时训练过程慢。
### 2.stochastic GD（随机梯度下降）
每次迭代仅对一个样本计算梯度，直到收敛。优点：训练速度快，每次只用一个样本，可用于online learning；缺点：准确度下降，遇上噪声易陷入局部最优。
### 3. mini-batch GD
>介于BGD和SGD之间的一种优化算法。每次选取一个批量进行迭代计算更新参数。
速度比BGD快，比SGD慢，精度比BGD低比SGD高。
* 缺点
  * 选择合适的learning rate比较困难 - 如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。
  * SGD容易收敛到局部最优。

## 2.SGDM(SGD+Momentum)
>为了抑制SGD的震荡，在原步长基础上加一个与以前梯度相关的累积项，距离现在越近的梯度权重越大。
* 特点
  * 下降初期，下降方向一致能加速训练；
  * 遇到局部最小值震荡时帮助跳出陷阱，抑制震荡，加快收敛。


## 3.SGD with Nesterov Acceleration
>先根据当前累积梯度计算下一个位置的梯度，用下个位置的梯度更新累积梯度，然后用更新的累积梯度进行当前的梯度下降。
* 特点：这种计算梯度的方式可以使算法更好的「预测未来」，提前调整更新速率。算法能够在目标函数有增高趋势之前，减缓更新速率。

## 4.Adagrad
>深度神经网络往往包含大量的参数，SGD及其变种以同样的学习率更新每个参数，对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。
* 特点：这一方法在稀疏数据场景下表现非常好。但也存在一些问题：因为正则项是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。因为无视了梯度的大小，这种方法对于参数的初始条件和对应的梯度大小可能是敏感的，初始梯度大的话，之后的学习速率会下降。但是可以通过增加全局学习率来缓解这种情况。需要人工选择学习率。

## 5.RMSprop
>在 Adagrad 中， 正则项惩罚是单调递增的，使得学习率逐渐递减至0，可能导致训练过程提前结束。为了改进这一缺点，可以考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间窗口内的下降梯度。（乘以衰减系数）
* 特点：
  * 其实RMSprop依然依赖于全局学习率。
  * RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间。
  * 适合处理非平稳目标-对于RNN效果很好。

# 6.Adadelta
* 优点：
  * 该方法不需要手动调整学习速率
  * 对超参数不敏感
  * 对大梯度，噪声，不同架构具有很好的健壮性
  * 相对Adagrad解决了学习率持续衰减和需要人工选择学习率。

## 6.Adam
>可以看成RMSprop和Momentum的结合。
* Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。

## 7.Nadam
> 在Adam上融合NAG的思想
